{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alpha-bracelet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from plotnine import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "marked-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrapped computation\n",
    "def computeLR(target_data, feature_headers, feature_data, bootstraps):\n",
    "\n",
    "    accuracies = list()\n",
    "    sensitivities = list()\n",
    "    specificities = list()\n",
    "\n",
    "    for _ in range(0,int(bootstraps)):\n",
    "        \n",
    "        train_x = \"\"\n",
    "        test_x = \"\"\n",
    "        train_y = \"\"\n",
    "        test_y = \"\"\n",
    "    \n",
    "        diff = 1\n",
    "    \n",
    "        while diff > 0.15:\n",
    "        \n",
    "            train_x, test_x, train_y, test_y = train_test_split(feature_data, target_data, train_size=0.4, test_size=0.6)\n",
    "            diff = abs(np.sum(train_y==0) - np.sum(train_y==1)) / len(train_y)\n",
    "        \n",
    "        logreg = LogisticRegression(max_iter = 1000)\n",
    "        \n",
    "        logreg.fit(train_x, train_y)\n",
    "        y_pred = logreg.predict(test_x)\n",
    "        accuracies.append(logreg.score(test_x, test_y))\n",
    "        \n",
    "    print(sum(accuracies)/len(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reverse-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLR_write(target_data, feature_headers, feature_data, bootstraps, modelName):\n",
    "\n",
    "    logreg_models = list()\n",
    "    \n",
    "    accuracies = list()\n",
    "    sensitivities = list() # TPR\n",
    "    specificities = list() # \n",
    "    FPR = list()\n",
    "    \n",
    "\n",
    "    for _ in range(0,int(bootstraps)):\n",
    "        \n",
    "        train_x = \"\"\n",
    "        test_x = \"\"\n",
    "        train_y = \"\"\n",
    "        test_y = \"\"\n",
    "    \n",
    "        diff = 1\n",
    "    \n",
    "        while diff > 0.15:\n",
    "        \n",
    "            train_x, test_x, train_y, test_y = train_test_split(feature_data, target_data, train_size=0.4, test_size=0.6)\n",
    "            diff = abs(np.sum(train_y==0) - np.sum(train_y==1)) / len(train_y)\n",
    "        \n",
    "        logreg = LogisticRegression(max_iter = 1000)\n",
    "        \n",
    "        logreg.fit(train_x, train_y)\n",
    "        logreg_models.append(logreg)\n",
    "        \n",
    "        y_pred = logreg.predict(test_x)\n",
    "        accuracies.append(logreg.score(test_x, test_y))\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(test_y, y_pred).ravel() # collapse to 1D array\n",
    "        sensitivities.append(tp/(tp+fn))\n",
    "        specificities.append(tn/(tn+fp))\n",
    "        FPR.append(fp/(fp+tn))\n",
    "       \n",
    "    print(sum(accuracies)/len(accuracies))\n",
    "\n",
    "    \n",
    "    with open(modelName+\"_metrics.txt\", \"w\") as OUT:\n",
    "        \n",
    "        OUT.write(\"endpoint_model\" + \"\\t\" + \"metric\" + \"\\t\" + \"acc_sens_spec\" + \"\\n\")\n",
    "        #OUT.write(\"endpoint_model\" + \"\\t\" + \"TPR\" + \"\\t\" + \"FPR\" + \"\\t\" + \"specificity\" + \"\\t\" + \"accuracy\" + \"\\n\")\n",
    "        for x in range(0,len(accuracies)):\n",
    "            #OUT.write(modelName + \"\\t\" + str(sensitivities[x]) + \"\\t\" + str(FPR[x]) + \"\\t\" + str(specificities[x]) + \"\\t\" + str(accuracies[x]) + \"\\n\")\n",
    "            OUT.write(modelName + \"\\t\" + str(accuracies[x]) + \"\\t\" + \"accuracy\" + \"\\n\")\n",
    "            OUT.write(modelName + \"\\t\" + str(sensitivities[x]) + \"\\t\" + \"sensitivity\" + \"\\n\")\n",
    "            OUT.write(modelName + \"\\t\" + str(specificities[x]) + \"\\t\" + \"specificity\" + \"\\n\")\n",
    "            \n",
    "    return(logreg_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raising-essence",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CR3M = pd.read_csv(\"dataset/HCC_TACE_metadata_combined_onehot_wTXomics_mod.txt\", sep = \"\\t\")\n",
    "dataset_CR3M = dataset_CR3M.dropna(axis='rows', how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developing-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_CR3M['Area_log10'] = np.log10(dataset_CR3M.loc[:,\"Area\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "differential-boundary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 323)\n",
      "['PatientID', 'BiopsyID_Tumor', 'BiopsyID_NonTumor', 'CR3M', 'Cohort', 'Sex', 'Age', 'Cirrhosis', 'BCLC', 'Child_Pugh', 'NumLiverDiseases', 'Liver.Disease', 'ALD', 'NAFLD', 'HCV', 'HBV', 'HDV', 'NASH', 'AFP_pre', 'AFP_post', 'num_nodules', 'LesionID', 'Segment', 'Edmondson', 'Area', 'Entropy', 'Kurtosis', 'Mpp', 'Skewness', 'Uniformity', 'outliers', 'NT_TU_DE_dist', 'TU_DE_PC1_dist', 'HES2', 'LOC100506022', 'STMN1', 'CLSPN', 'CDCA8', 'CLDN19', 'CDC20', 'KIF2C', 'RAD54L', 'STIL', 'ORC1', 'IL23R', 'DEPDC1', 'PTBP2', 'SASS6', 'RP11-305E17.6', 'GPSM2', 'TRIM45', 'FAM72C', 'FAM72D', 'RNVU1-6', 'LINC00624', 'HIST2H2BE', 'CD1A', 'ITLN2', 'HSPA6', 'NUF2', 'ANKRD45', 'ASPM', 'KIF14', 'UBE2T', 'FAM72A', 'NEK2', 'DTL', 'CENPF', 'HHIPL2', 'EXO1', 'LINC00200', 'MCM10', 'ACBD7', 'LOC101929475', 'ZWINT', 'CDK1', 'DYDC2', 'KIF11', 'HELLS', 'LOXL4', 'CALHM2', 'ADAM12', 'MKI67', 'RRM1', 'USH1C', 'OTOG', 'SAA4', 'SAA2', 'SAA1', 'E2F8', 'KIF18A', 'CD44', 'SLC35C1', 'FAM111B', 'CDCA5', 'POLD3', 'APOA4', 'CHEK1', 'FOXM1', 'RAD51AP1', 'CDCA3', 'GUCY2C', 'TROAP', 'AQP6', 'RACGAP1', 'ESPL1', 'MYO1A', 'CSRP2', 'E2F7', 'NTS', 'GAS2L3', 'PARPBP', 'CIT', 'SKA3', 'BRCA2', 'RFC3', 'POSTN', 'DIAPH3', 'CDKN3', 'WDHD1', 'DLGAP5', 'FOS', 'GPR68', 'ARHGAP11A', 'BUB1B', 'CASC5', 'RAD51', 'OIP5', 'NUSAP1', 'WDR76', 'FGF7', 'LOC102724683', 'CCNB2', 'KIAA0101', 'IQCH', 'KIF23', 'STRA6', 'CHRNA5', 'FANCI', 'TICRR', 'PRC1', 'TIGD7', 'TMC5', 'LOC81691', 'MT1H', 'CDH11', 'GINS2', 'CDH15', 'DPEP1', 'NXN', 'RNASEK', 'SCARNA21', 'ATAD5', 'CCL13', 'CDC6', 'TOP2A', 'BRCA1', 'ARL4D', 'DBF4B', 'KIF18B', 'PRR11', 'BRIP1', 'LINC00673', 'EVPL', 'ST6GALNAC1', 'BIRC5', 'SOCS3', 'TYMS', 'NDC80', 'RBBP8', 'SLC14A2', 'SKA1', 'SPC24', 'ZNF826P', 'CEACAM21', 'OSCAR', 'RRM2', 'CENPA', 'MORN2', 'LOC102723845', 'MSH2', 'FAM161A', 'CD207', 'CYP26B1', 'REG3A', 'NCAPH', 'BUB1', 'CKAP2L', 'MCM6', 'KCNJ3', 'SPC25', 'HOXD10', 'SGOL2', 'TMEM237', 'BARD1', 'AC072062.1', 'TM4SF20', 'SLC16A14', 'MROH2A', 'HJURP', 'MCM8', 'FERMT1', 'CST1', 'GINS1', 'TPX2', 'DSN1', 'RP4-640H8.2', 'TGM2', 'LBP', 'MYBL2', 'DOK5', 'AURKA', 'CHAF1B', 'COL6A1', 'S100B', 'CDC45', 'BMS1P20', 'HMOX1', 'MFNG', 'DMC1', 'RIBC2', 'GTSE1', 'DENND6B', 'FANCD2', 'FBLN2', 'SGOL1', 'ZNF620', 'KIF15', 'CDC25A', 'C3orf67', 'KIAA1524', 'POLQ', 'ECT2', 'RFC4', 'ZNF732', 'NCAPG', 'LGI2', 'LOC102723415', 'PARM1', 'CXCL13', 'FRAS1', 'RP11-696N14.1', 'CENPE', 'MAD2L1', 'CCNA2', 'PLK4', 'TMEM154', 'HMGB2', 'NEIL3', 'CENPU', 'TRIP13', 'C5orf34', 'ESM1', 'PART1', 'CENPK', 'CCNB1', 'DHFR', 'LMNB1', 'KIF20A', 'CDC25C', 'MIR143HG', 'ADAM19', 'PTTG1', 'ATP10B', 'HMMR', 'COL23A1', 'ADTRP', 'LOC101928433', 'SCGN', 'HIST1H1A', 'HIST1H2BB', 'TCF19', 'LOC100294145', 'KIFC1', 'PACSIN1', 'SPDEF', 'CENPQ', 'LOC730101', 'GSTA4', 'TTK', 'SIM1', 'RPA3', 'CDCA7L', 'KLHL7-AS1', 'MPP6', 'ANLN', 'GCK', 'IGFBP1', 'FIGNL1', 'CCL24', 'PEG10', 'MCM7', 'EZH2', 'WDR86-AS1', 'XRCC2', 'NCAPG2', 'CDCA2', 'ESCO2', 'PBK', 'LOC100505501', 'TRPA1', 'CCNE2', 'FBXO43', 'RSPO2', 'FAM83A-AS1', 'MELK', 'IDNK', 'ZNF367', 'ZNF883', 'TRNL1', 'TRNW', 'MID1_2', 'IL1RAPL1', 'TIMP1', 'PAGE4', 'KIF4A', 'ERCC6L', 'SRPX2', 'CENPI', 'CXorf57', 'VSIG1', 'HPRT1', 'Area_log10']\n"
     ]
    }
   ],
   "source": [
    "print(dataset_CR3M.shape)\n",
    "print(list(dataset_CR3M.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foster-trash",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    24\n",
       "1    17\n",
       "Name: CR3M, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_CR3M[\"CR3M\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "equal-philip",
   "metadata": {},
   "outputs": [],
   "source": [
    "discovery_dataset = dataset_CR3M[~dataset_CR3M['BiopsyID_Tumor'].isin([\"D212\",\"D601\",\"D999\",\"E001\",\"E096\",\"E185\",\"E209\",\"D180\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "increasing-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = dataset_CR3M[dataset_CR3M['BiopsyID_Tumor'].isin([\"D212\",\"D601\",\"D999\",\"E001\",\"E096\",\"E185\",\"E209\",\"D180\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fossil-jonathan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7920000000000019\n"
     ]
    }
   ],
   "source": [
    "target_data = np.array(discovery_dataset.loc[:,'CR3M']).ravel() # ravel converts to 1D array\n",
    "feature_headers = ['Area_log10']\n",
    "feature_data = np.array(preprocessing.scale(discovery_dataset.loc[:,feature_headers]))\n",
    "\n",
    "computeLR(target_data, feature_headers, feature_data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cubic-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8528999999999994\n"
     ]
    }
   ],
   "source": [
    "target_data = np.array(discovery_dataset.loc[:,'CR3M']).ravel() # ravel converts to 1D array\n",
    "feature_headers = ['FAM111B','HPRT1']\n",
    "feature_data = np.array(preprocessing.scale(discovery_dataset.loc[:,feature_headers]))\n",
    "\n",
    "computeLR(target_data, feature_headers, feature_data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acceptable-dealing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8977999999999972\n"
     ]
    }
   ],
   "source": [
    "target_data = np.array(discovery_dataset.loc[:,'CR3M']).ravel() # ravel converts to 1D array\n",
    "feature_headers = ['Area_log10','FAM111B','HPRT1']\n",
    "feature_data = np.array(preprocessing.scale(discovery_dataset.loc[:,feature_headers]))\n",
    "\n",
    "computeLR(target_data, feature_headers, feature_data, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aboriginal-lafayette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 1 1 0]\n",
      "[1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "target_data = np.array(discovery_dataset.loc[:,'CR3M']).ravel() # ravel converts to 1D array\n",
    "feature_headers = ['Area_log10','FAM111B','HPRT1']\n",
    "feature_data = np.array(preprocessing.scale(discovery_dataset.loc[:,feature_headers]))\n",
    "\n",
    "train_x = \"\"\n",
    "test_x = \"\"\n",
    "train_y = \"\"\n",
    "test_y = \"\"\n",
    "\n",
    "diff = 1\n",
    "    \n",
    "while diff > 0.15:\n",
    "        \n",
    "    train_x, test_x, train_y, test_y = train_test_split(feature_data, target_data, train_size=0.4, test_size=0.6)\n",
    "    diff = abs(np.sum(train_y==0) - np.sum(train_y==1)) / len(train_y)\n",
    "        \n",
    "logreg = LogisticRegression(max_iter = 1000)\n",
    "        \n",
    "logreg.fit(train_x, train_y)\n",
    "y_pred = logreg.predict(test_x)\n",
    "\n",
    "print(test_y)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "guilty-chosen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D180' 'E209' 'D999' 'E001' 'D212' 'D601' 'E185' 'E096']\n",
      "[0 0 0 0 1 1 1 1]\n",
      "[0 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# this can also be bootstrapped: E001 is the only sample which is consistently misclassified\n",
    "\n",
    "validation_target = np.array(validation_dataset.loc[:,'CR3M']).ravel() # ravel converts to 1D array\n",
    "validation_features = ['Area_log10','FAM111B','HPRT1']\n",
    "validation_data = np.array(preprocessing.scale(validation_dataset.loc[:,validation_features]))\n",
    "\n",
    "validation_pred = logreg.predict(validation_data)\n",
    "\n",
    "#print(np.array(validation_dataset.loc[:,'TumorID.x']).ravel())\n",
    "print(np.array(validation_dataset.loc[:,'BiopsyID_Tumor']).ravel())\n",
    "print(validation_target)\n",
    "print(validation_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the above cell for new data, provided it is preprocessed along with the discovery cohort samples as described in the manuscript"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
